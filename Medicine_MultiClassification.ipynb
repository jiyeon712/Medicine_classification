{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyeon712/Medicine_classification/blob/main/Medicine_MultiClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-F1z1Y7833",
        "outputId": "17a4a51d-e09a-48cb-fb53-2f91e83a6597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 별 정확도\n",
        "- Cnn with vgg 16 : 99.4%\n",
        "- Resnet : 99.01%\n",
        "- Cnn : 98.68%"
      ],
      "metadata": {
        "id": "1moK6yfCP6xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "sB_WRTA79n9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# 파일 경로 설정\n",
        "image_files = glob.glob(\"/content/drive/MyDrive/MedicineContest/drug&medicine/*/*\")\n",
        "\n",
        "# 폴더명 추출\n",
        "folder_names = {os.path.basename(os.path.dirname(file_path)) for file_path in image_files}\n",
        "\n",
        "print('파일명 : ', folder_names)\n",
        "print('파일 개수 :' ,len(folder_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p6akh8N-GNF",
        "outputId": "9c1483d7-01af-4203-9905-d9c2cf3d37b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일명 :  {'28 Methylenedioxypyrovalerone,MDPV', '259 Diazepam', '282 5-MeO-DiPT', '40122 Uristat', '258 Lorazepam', '11 Locaserin', '238 Mexazolam', '40767 Gestaren Tab', '224 Bromazepam', '128 Pentazocine', '114 Flurazepam', '139 Fenetylline', '162 Carisoprodol', '9 AL-LAD', '41107 Anacox Cap', '157 Kratom', '40720 New Glia Tab', '1 Reminazolam', '169 Zopiclone', '166 Zipeprol', '156 Clonazepam', '22 Etizolam', '168 Zolpidem', '193 Ethyl Loflazepate', '59 4,4_DMAR', '40990 Sebaco Hct Tab', '30 W-18', '41207 Vtamin Tab', '170 Ibogaine', '161 Camazepam', '41172 Levetiracetam', '41170 Leviepill Tab', '303 Phenibut', '137 Pemoline', '29002 Opiqutan Soft Cap', '40953 Canagabarotin Cap', '152 Chlordiazepoxide', '107 Halazepam', '195 Ethchlorvynol', '34342 Welood Soft Cap', '295 2C-B', '245 Methylphenidate', '40792 Diacell Cap', '253 Mazindol', '40949 Afental CR Tab', '37990 Multi-Q10', '147 Temazepam', '41327 Rabeprazole Sodium', '94 5-mapb', '78 Phenazepam', '40991 Sebaco Hct Tab', '41169 Razarect Tab', '198 Alprazolam', '40837 Duosta Tab', '138 Phenobarbital', '239 Meprobamate', '39916 Uniteride Tab', '41097 Kyungdong Fanitine Tab', '41344 Tovast Tab', '12 Quazepam', '155 Clorazepate', '233 Benzphetamine', '150 Clobazam', '116 Flunitrazepam', '149 Clotiazepam', '194 Estazolam', '133 Phendimetrazine', '144 Triazolam'}\n",
            "파일 개수 : 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 이미지 파일 경로 설정\n",
        "image_files = glob.glob(\"/content/drive/MyDrive/MedicineContest/drug&medicine/*/*\")\n",
        "\n",
        "# 이미지와 라벨을 저장할 리스트 생성\n",
        "images = []  # 이미지 데이터를 저장할 리스트\n",
        "labels = []  # 라벨 데이터를 저장할 리스트\n",
        "\n",
        "# 이미지 파일을 읽어서 이미지 데이터(images)와 라벨 데이터(labels)에 저장\n",
        "for file_path in image_files:\n",
        "    # 이미지 파일의 경로에서 라벨(폴더 이름)을 추출\n",
        "    label = os.path.basename(os.path.dirname(file_path))\n",
        "    # 이미지 파일 읽기\n",
        "    img = cv2.imread(file_path)\n",
        "    # 이미지 크기 조정 (선택적)\n",
        "    img = cv2.resize(img, (150, 150))\n",
        "    # 이미지 데이터(images)와 라벨 데이터(labels)에 추가\n",
        "    images.append(img)\n",
        "    labels.append(label)\n",
        "\n",
        "# 리스트를 넘파이 배열로 변환\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# 데이터셋의 크기 확인\n",
        "print(\"이미지 데이터의 크기:\", images.shape)\n",
        "print(\"라벨 데이터의 크기:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky3ExaQ8-fNK",
        "outputId": "437b2124-73a4-479d-f89a-0e0ef7c878bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이미지 데이터의 크기: (3796, 150, 150, 3)\n",
            "라벨 데이터의 크기: (3796,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 라벨 인코더 생성\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 라벨 데이터를 숫자 형태로 변환\n",
        "labels_encoded = label_encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "urwzzij9yzrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test로 구분하기"
      ],
      "metadata": {
        "id": "EMWlGxDpy7zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)\n",
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NuRSa4ay18r",
        "outputId": "6def561a-c6d4-4fc5-f63b-7f32624e339a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3036, 150, 150, 3), (3036,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설계"
      ],
      "metadata": {
        "id": "FFXS3fI6QF7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "4oM_M8TYTSqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled=x_train/255.0\n",
        "x_test_scaled=x_test/255.0\n",
        "\n",
        "### DNN 모델에서는 실행해야함.\n",
        "# x_train_scaled = x_train_scaled.reshape(-1,150*150*3)\n",
        "# x_test_scaled=x_test_scaled.reshape(-1,150*150*3)"
      ],
      "metadata": {
        "id": "JJMnmY__y2UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Sequential, Input\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "iJnAy_sKSYVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape = (150,150,3)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, kernel_size = 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "# 세 번째 컨볼루션 층 추가 : 256 필터\n",
        "model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "# 네 번째 컨볼루션 층: 512 필터\n",
        "model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense 층: 100 -> 200 노드로 증가\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(rate=0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(68, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Dp48QyQER1",
        "outputId": "78a86c9b-50d2-4296-86bc-0ffe892abefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 150, 150, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 75, 75, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 37, 37, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 37, 37, 256)       147712    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 18, 18, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 9, 9, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 41472)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               8294600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 68)                13668     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9655532 (36.83 MB)\n",
            "Trainable params: 9655532 (36.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 모델 학습"
      ],
      "metadata": {
        "id": "4cjYccbyTPSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dT6Bs64ZUCi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "history = model.fit(x_train_scaled, y_train,\n",
        "                    validation_split=0.3,\n",
        "                    batch_size=32,\n",
        "                    epochs=100,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNNoRXMFTNgz",
        "outputId": "a8b90bba-7101-4734-be9a-73b8d21276b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "67/67 [==============================] - 13s 89ms/step - loss: 4.0665 - accuracy: 0.0480 - val_loss: 3.2736 - val_accuracy: 0.1131\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 3s 41ms/step - loss: 2.9593 - accuracy: 0.1991 - val_loss: 2.4451 - val_accuracy: 0.3304\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 2.2358 - accuracy: 0.3713 - val_loss: 1.6742 - val_accuracy: 0.4852\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 3s 41ms/step - loss: 1.7248 - accuracy: 0.4955 - val_loss: 1.2533 - val_accuracy: 0.6136\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 1.3419 - accuracy: 0.5962 - val_loss: 1.0730 - val_accuracy: 0.6542\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 3s 48ms/step - loss: 1.0960 - accuracy: 0.6612 - val_loss: 0.9796 - val_accuracy: 0.6948\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 0.9225 - accuracy: 0.7285 - val_loss: 0.6356 - val_accuracy: 0.7870\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 3s 41ms/step - loss: 0.6939 - accuracy: 0.7760 - val_loss: 0.4519 - val_accuracy: 0.8584\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 3s 41ms/step - loss: 0.6243 - accuracy: 0.8160 - val_loss: 0.6081 - val_accuracy: 0.8002\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.4951 - accuracy: 0.8348 - val_loss: 0.3315 - val_accuracy: 0.9045\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.4228 - accuracy: 0.8598 - val_loss: 0.3398 - val_accuracy: 0.8979\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.3980 - accuracy: 0.8781 - val_loss: 0.2397 - val_accuracy: 0.9374\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.2980 - accuracy: 0.8927 - val_loss: 0.3295 - val_accuracy: 0.8858\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 0.3207 - accuracy: 0.8998 - val_loss: 0.3233 - val_accuracy: 0.9089\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 3s 50ms/step - loss: 0.2608 - accuracy: 0.9129 - val_loss: 0.2574 - val_accuracy: 0.9144\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.3018 - accuracy: 0.9096 - val_loss: 0.7677 - val_accuracy: 0.8090\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.2223 - accuracy: 0.9327 - val_loss: 0.1588 - val_accuracy: 0.9572\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.2521 - accuracy: 0.9242 - val_loss: 0.1712 - val_accuracy: 0.9451\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.1882 - accuracy: 0.9393 - val_loss: 0.1844 - val_accuracy: 0.9473\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.1614 - accuracy: 0.9421 - val_loss: 0.1763 - val_accuracy: 0.9539\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 3s 49ms/step - loss: 0.1612 - accuracy: 0.9506 - val_loss: 0.3427 - val_accuracy: 0.9078\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 0.2607 - accuracy: 0.9374 - val_loss: 0.1824 - val_accuracy: 0.9517\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 0.1357 - accuracy: 0.9572 - val_loss: 0.3807 - val_accuracy: 0.9232\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 0.1742 - accuracy: 0.9501 - val_loss: 0.1372 - val_accuracy: 0.9671\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 3s 48ms/step - loss: 0.1017 - accuracy: 0.9661 - val_loss: 0.1330 - val_accuracy: 0.9693\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.1443 - accuracy: 0.9553 - val_loss: 0.1645 - val_accuracy: 0.9605\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 3s 42ms/step - loss: 0.1239 - accuracy: 0.9661 - val_loss: 0.1405 - val_accuracy: 0.9616\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.1288 - accuracy: 0.9609 - val_loss: 0.1420 - val_accuracy: 0.9594\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.1185 - accuracy: 0.9675 - val_loss: 0.0904 - val_accuracy: 0.9802\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 3s 50ms/step - loss: 0.0764 - accuracy: 0.9765 - val_loss: 0.8029 - val_accuracy: 0.8364\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0878 - accuracy: 0.9760 - val_loss: 0.1606 - val_accuracy: 0.9605\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.1531 - accuracy: 0.9572 - val_loss: 0.1972 - val_accuracy: 0.9539\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0778 - accuracy: 0.9760 - val_loss: 0.1038 - val_accuracy: 0.9769\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.1140 - accuracy: 0.9704 - val_loss: 0.1324 - val_accuracy: 0.9704\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 3s 49ms/step - loss: 0.1191 - accuracy: 0.9671 - val_loss: 0.1158 - val_accuracy: 0.9813\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0828 - accuracy: 0.9732 - val_loss: 0.2127 - val_accuracy: 0.9506\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.1109 - accuracy: 0.9732 - val_loss: 0.1325 - val_accuracy: 0.9726\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0609 - accuracy: 0.9826 - val_loss: 0.0992 - val_accuracy: 0.9759\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 3s 50ms/step - loss: 0.0738 - accuracy: 0.9774 - val_loss: 0.1540 - val_accuracy: 0.9572\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.0865 - accuracy: 0.9774 - val_loss: 0.1328 - val_accuracy: 0.9660\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0768 - accuracy: 0.9769 - val_loss: 0.1847 - val_accuracy: 0.9627\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0581 - accuracy: 0.9779 - val_loss: 0.0997 - val_accuracy: 0.9769\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.0695 - accuracy: 0.9802 - val_loss: 0.1420 - val_accuracy: 0.9748\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0610 - accuracy: 0.9845 - val_loss: 0.1478 - val_accuracy: 0.9649\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0624 - accuracy: 0.9816 - val_loss: 0.1060 - val_accuracy: 0.9824\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0633 - accuracy: 0.9774 - val_loss: 0.4532 - val_accuracy: 0.9122\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 3s 49ms/step - loss: 0.0649 - accuracy: 0.9831 - val_loss: 0.1331 - val_accuracy: 0.9726\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0671 - accuracy: 0.9831 - val_loss: 0.1048 - val_accuracy: 0.9802\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 0.0562 - accuracy: 0.9887 - val_loss: 0.1463 - val_accuracy: 0.9693\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0546 - accuracy: 0.9845 - val_loss: 0.1165 - val_accuracy: 0.9748\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0715 - accuracy: 0.9826 - val_loss: 0.1313 - val_accuracy: 0.9802\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0648 - accuracy: 0.9864 - val_loss: 0.1687 - val_accuracy: 0.9693\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0753 - accuracy: 0.9831 - val_loss: 0.1143 - val_accuracy: 0.9835\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0687 - accuracy: 0.9873 - val_loss: 0.1416 - val_accuracy: 0.9780\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.1146 - accuracy: 0.9802 - val_loss: 0.1566 - val_accuracy: 0.9682\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0677 - accuracy: 0.9831 - val_loss: 0.1033 - val_accuracy: 0.9835\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 3s 48ms/step - loss: 0.0567 - accuracy: 0.9849 - val_loss: 0.1770 - val_accuracy: 0.9693\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 0.0897 - accuracy: 0.9774 - val_loss: 0.3553 - val_accuracy: 0.9484\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0519 - accuracy: 0.9887 - val_loss: 0.1056 - val_accuracy: 0.9802\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0373 - accuracy: 0.9901 - val_loss: 0.1008 - val_accuracy: 0.9857\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0864 - accuracy: 0.9802 - val_loss: 0.1052 - val_accuracy: 0.9802\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 3s 50ms/step - loss: 0.0507 - accuracy: 0.9854 - val_loss: 0.1960 - val_accuracy: 0.9748\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.0187 - accuracy: 0.9944 - val_loss: 0.1368 - val_accuracy: 0.9791\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0304 - accuracy: 0.9901 - val_loss: 0.1701 - val_accuracy: 0.9704\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0372 - accuracy: 0.9901 - val_loss: 0.1650 - val_accuracy: 0.9726\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 3s 48ms/step - loss: 0.1148 - accuracy: 0.9816 - val_loss: 0.1319 - val_accuracy: 0.9769\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0316 - accuracy: 0.9878 - val_loss: 2.7623 - val_accuracy: 0.7398\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.1163 - accuracy: 0.9812 - val_loss: 4.0453 - val_accuracy: 0.6389\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.1263 - accuracy: 0.9802 - val_loss: 0.1336 - val_accuracy: 0.9791\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.2058 - val_accuracy: 0.9550\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 0.0441 - accuracy: 0.9896 - val_loss: 0.1320 - val_accuracy: 0.9791\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 3s 49ms/step - loss: 0.0612 - accuracy: 0.9887 - val_loss: 0.3371 - val_accuracy: 0.9495\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0555 - accuracy: 0.9854 - val_loss: 0.2330 - val_accuracy: 0.9583\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0314 - accuracy: 0.9906 - val_loss: 0.1185 - val_accuracy: 0.9791\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0636 - accuracy: 0.9859 - val_loss: 0.1357 - val_accuracy: 0.9857\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0676 - accuracy: 0.9831 - val_loss: 0.1372 - val_accuracy: 0.9780\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0484 - accuracy: 0.9892 - val_loss: 0.1358 - val_accuracy: 0.9769\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0931 - accuracy: 0.9849 - val_loss: 0.1099 - val_accuracy: 0.9802\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0316 - accuracy: 0.9915 - val_loss: 0.1509 - val_accuracy: 0.9791\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 3s 48ms/step - loss: 0.0531 - accuracy: 0.9882 - val_loss: 0.1151 - val_accuracy: 0.9868\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0512 - accuracy: 0.9896 - val_loss: 0.0837 - val_accuracy: 0.9857\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0500 - accuracy: 0.9882 - val_loss: 0.1067 - val_accuracy: 0.9857\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0389 - accuracy: 0.9915 - val_loss: 0.1237 - val_accuracy: 0.9813\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0251 - accuracy: 0.9911 - val_loss: 0.1247 - val_accuracy: 0.9857\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.0783 - val_accuracy: 0.9879\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.0319 - accuracy: 0.9929 - val_loss: 0.1197 - val_accuracy: 0.9791\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0652 - accuracy: 0.9882 - val_loss: 0.1597 - val_accuracy: 0.9780\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0705 - accuracy: 0.9887 - val_loss: 0.1007 - val_accuracy: 0.9868\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0263 - accuracy: 0.9915 - val_loss: 0.2919 - val_accuracy: 0.9616\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0625 - accuracy: 0.9849 - val_loss: 0.1356 - val_accuracy: 0.9769\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0347 - accuracy: 0.9882 - val_loss: 0.1825 - val_accuracy: 0.9769\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0662 - accuracy: 0.9821 - val_loss: 0.1252 - val_accuracy: 0.9813\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0469 - accuracy: 0.9920 - val_loss: 0.1084 - val_accuracy: 0.9846\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 3s 45ms/step - loss: 0.0254 - accuracy: 0.9906 - val_loss: 0.1211 - val_accuracy: 0.9802\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0387 - accuracy: 0.9911 - val_loss: 0.1265 - val_accuracy: 0.9802\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 3s 49ms/step - loss: 0.0537 - accuracy: 0.9896 - val_loss: 0.1472 - val_accuracy: 0.9802\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 3s 47ms/step - loss: 0.0288 - accuracy: 0.9911 - val_loss: 0.1298 - val_accuracy: 0.9857\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 3s 43ms/step - loss: 0.0654 - accuracy: 0.9849 - val_loss: 0.2176 - val_accuracy: 0.9759\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 3s 46ms/step - loss: 0.0605 - accuracy: 0.9887 - val_loss: 0.2163 - val_accuracy: 0.9791\n",
            "Epoch 100/100\n",
            "67/67 [==============================] - 3s 44ms/step - loss: 0.1144 - accuracy: 0.9864 - val_loss: 0.1317 - val_accuracy: 0.9835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "model.evaluate(x_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "LCZLzQdqhyby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ec4c90-69f7-4d2c-e7f4-bc54db71700e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 43ms/step - loss: 0.0718 - accuracy: 0.9868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07182274013757706, 0.9868420958518982]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 모델 예측 및 평가 지표 계산\n",
        "y_pred = model.predict(x_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylJjBxVQySsx",
        "outputId": "ee5d87c9-afab-4883-a1d4-121ef04bcea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 12ms/step\n",
            "Confusion Matrix:\n",
            " [[ 8  0  0 ...  0  0  0]\n",
            " [ 0  5  0 ...  0  0  0]\n",
            " [ 0  0 23 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  7  0  0]\n",
            " [ 0  0  0 ...  0  5  0]\n",
            " [ 0  0  0 ...  0  0  4]]\n",
            "Classification Report:\n",
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "                     1 Reminazolam       1.00      1.00      1.00         8\n",
            "                     107 Halazepam       1.00      0.71      0.83         7\n",
            "                      11 Locaserin       1.00      1.00      1.00        23\n",
            "                    114 Flurazepam       1.00      1.00      1.00         5\n",
            "                 116 Flunitrazepam       0.94      1.00      0.97        15\n",
            "                       12 Quazepam       1.00      1.00      1.00        36\n",
            "                   128 Pentazocine       1.00      1.00      1.00        10\n",
            "               133 Phendimetrazine       1.00      1.00      1.00        19\n",
            "                      137 Pemoline       1.00      1.00      1.00        13\n",
            "                 138 Phenobarbital       1.00      1.00      1.00         4\n",
            "                   139 Fenetylline       1.00      1.00      1.00        12\n",
            "                     144 Triazolam       0.92      1.00      0.96        12\n",
            "                     147 Temazepam       1.00      1.00      1.00        13\n",
            "                   149 Clotiazepam       1.00      1.00      1.00         8\n",
            "                      150 Clobazam       1.00      1.00      1.00         8\n",
            "              152 Chlordiazepoxide       1.00      1.00      1.00        17\n",
            "                   155 Clorazepate       1.00      1.00      1.00        10\n",
            "                    156 Clonazepam       1.00      1.00      1.00         6\n",
            "                        157 Kratom       1.00      1.00      1.00         3\n",
            "                     161 Camazepam       0.94      0.89      0.92        19\n",
            "                  162 Carisoprodol       1.00      1.00      1.00        13\n",
            "                      166 Zipeprol       1.00      1.00      1.00         4\n",
            "                      168 Zolpidem       0.69      0.82      0.75        11\n",
            "                     169 Zopiclone       1.00      1.00      1.00        13\n",
            "                      170 Ibogaine       1.00      1.00      1.00        14\n",
            "             193 Ethyl Loflazepate       1.00      1.00      1.00        11\n",
            "                     194 Estazolam       1.00      1.00      1.00        13\n",
            "                 195 Ethchlorvynol       1.00      1.00      1.00        12\n",
            "                    198 Alprazolam       1.00      1.00      1.00        16\n",
            "                       22 Etizolam       1.00      1.00      1.00        38\n",
            "                    224 Bromazepam       1.00      1.00      1.00        16\n",
            "                 233 Benzphetamine       1.00      1.00      1.00         9\n",
            "                     238 Mexazolam       1.00      1.00      1.00         5\n",
            "                   239 Meprobamate       1.00      1.00      1.00        15\n",
            "               245 Methylphenidate       1.00      1.00      1.00        15\n",
            "                      253 Mazindol       1.00      1.00      1.00        21\n",
            "                     258 Lorazepam       1.00      1.00      1.00        20\n",
            "                      259 Diazepam       1.00      1.00      1.00        12\n",
            "28 Methylenedioxypyrovalerone,MDPV       1.00      0.80      0.89         5\n",
            "                    282 5-MeO-DiPT       1.00      1.00      1.00        14\n",
            "           29002 Opiqutan Soft Cap       1.00      1.00      1.00         3\n",
            "                          295 2C-B       1.00      1.00      1.00         5\n",
            "                           30 W-18       1.00      1.00      1.00        17\n",
            "                      303 Phenibut       1.00      1.00      1.00         9\n",
            "             34342 Welood Soft Cap       1.00      1.00      1.00         8\n",
            "                   37990 Multi-Q10       1.00      1.00      1.00        16\n",
            "               39916 Uniteride Tab       1.00      1.00      1.00         9\n",
            "                     40122 Uristat       1.00      1.00      1.00         9\n",
            "                40720 New Glia Tab       1.00      1.00      1.00        12\n",
            "                40767 Gestaren Tab       1.00      1.00      1.00         7\n",
            "                 40792 Diacell Cap       1.00      1.00      1.00         8\n",
            "                  40837 Duosta Tab       0.88      1.00      0.93         7\n",
            "              40949 Afental CR Tab       0.78      0.88      0.82         8\n",
            "           40953 Canagabarotin Cap       1.00      1.00      1.00        13\n",
            "              40990 Sebaco Hct Tab       1.00      1.00      1.00         9\n",
            "              40991 Sebaco Hct Tab       1.00      1.00      1.00        10\n",
            "      41097 Kyungdong Fanitine Tab       1.00      1.00      1.00         6\n",
            "                  41107 Anacox Cap       1.00      0.83      0.91         6\n",
            "                41169 Razarect Tab       1.00      1.00      1.00         9\n",
            "               41170 Leviepill Tab       1.00      1.00      1.00         7\n",
            "               41172 Levetiracetam       1.00      0.83      0.91         6\n",
            "                  41207 Vtamin Tab       1.00      1.00      1.00         8\n",
            "          41327 Rabeprazole Sodium       1.00      1.00      1.00         5\n",
            "                  41344 Tovast Tab       1.00      1.00      1.00        10\n",
            "                       59 4,4_DMAR       1.00      1.00      1.00        12\n",
            "                     78 Phenazepam       1.00      1.00      1.00         7\n",
            "                          9 AL-LAD       1.00      1.00      1.00         5\n",
            "                         94 5-mapb       1.00      1.00      1.00         4\n",
            "\n",
            "                          accuracy                           0.99       760\n",
            "                         macro avg       0.99      0.98      0.98       760\n",
            "                      weighted avg       0.99      0.99      0.99       760\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Macro Average (macro avg):\n",
        "\n",
        "정의: 모든 클래스의 지표를 계산한 후, 클래스의 비율을 고려하지 않고 단순 평균을 구함\n",
        "특징: 각 클래스의 중요도를 동일하게 취급. 클래스 불균형이 있는 데이터셋에서는 소수 클래스의 성능에 더 민감하게 반응.\n",
        "\n",
        "2. Weighted Average (weighted avg):\n",
        "\n",
        "정의: 각 클래스의 지표를 계산한 후, 해당 클래스의 샘플 수를 가중치로 하여 가중 평균을 구함\n",
        "특징: 클래스의 불균형을 고려. 즉, 샘플 수가 많은 클래스가 전체 평균에 더 큰 영향을 미침.\n"
      ],
      "metadata": {
        "id": "H1RXqQ4M02SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vgg16 transfer"
      ],
      "metadata": {
        "id": "BuuQ_5q5WEDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras\n",
        "\n",
        "# VGG모델 불러오기\n",
        "vgg_base = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
        "\n",
        "vgg_base.summary()"
      ],
      "metadata": {
        "id": "67lj1IE5WL8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0407a89-fd95-4f62-e3b2-1796a70b5d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vgg 모델 설계"
      ],
      "metadata": {
        "id": "wq1zF8ObXMAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "# 모델 생성\n",
        "model_vgg = Sequential()\n",
        "model_vgg.add(vgg_base)\n",
        "model_vgg.add(layers.Flatten())\n",
        "model_vgg.add(layers.Dense(256, activation='relu'))\n",
        "model_vgg.add(layers.Dropout(0.5))\n",
        "model_vgg.add(layers.Dense(68, activation='softmax'))"
      ],
      "metadata": {
        "id": "qZCo-lrwW1bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16의 Convolutional Layer는 이미 학습되어 있으므로 학습되지 않도록 설정\n",
        "vgg_base.trainable = False"
      ],
      "metadata": {
        "id": "HcJE6Qg9XLT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model_vgg.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "history_vgg = model_vgg.fit(x_train_scaled, y_train, batch_size=32, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "6YtCO50AXbJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935353e2-1f3a-4b5e-cf19-2f664316a6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "67/67 [==============================] - 17s 187ms/step - loss: 3.3009 - accuracy: 0.2296 - val_loss: 2.0815 - val_accuracy: 0.4577\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 1.7558 - accuracy: 0.5125 - val_loss: 1.0536 - val_accuracy: 0.7355\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 1.1479 - accuracy: 0.6696 - val_loss: 0.6887 - val_accuracy: 0.7925\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 7s 110ms/step - loss: 0.8339 - accuracy: 0.7544 - val_loss: 0.5807 - val_accuracy: 0.8463\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 0.6154 - accuracy: 0.8198 - val_loss: 0.3924 - val_accuracy: 0.8771\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 7s 108ms/step - loss: 0.4665 - accuracy: 0.8522 - val_loss: 0.2328 - val_accuracy: 0.9451\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.3776 - accuracy: 0.8847 - val_loss: 0.1955 - val_accuracy: 0.9594\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.3245 - accuracy: 0.8993 - val_loss: 0.1551 - val_accuracy: 0.9682\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 7s 105ms/step - loss: 0.2830 - accuracy: 0.9035 - val_loss: 0.1316 - val_accuracy: 0.9671\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.2325 - accuracy: 0.9214 - val_loss: 0.1205 - val_accuracy: 0.9682\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 6s 94ms/step - loss: 0.2156 - accuracy: 0.9374 - val_loss: 0.1264 - val_accuracy: 0.9627\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.2001 - accuracy: 0.9384 - val_loss: 0.1058 - val_accuracy: 0.9715\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.1811 - accuracy: 0.9464 - val_loss: 0.0909 - val_accuracy: 0.9671\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.1636 - accuracy: 0.9440 - val_loss: 0.0696 - val_accuracy: 0.9748\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.1522 - accuracy: 0.9454 - val_loss: 0.0603 - val_accuracy: 0.9769\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.1238 - accuracy: 0.9581 - val_loss: 0.0534 - val_accuracy: 0.9813\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.1372 - accuracy: 0.9572 - val_loss: 0.0658 - val_accuracy: 0.9780\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.1107 - accuracy: 0.9652 - val_loss: 0.0725 - val_accuracy: 0.9748\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.1007 - accuracy: 0.9628 - val_loss: 0.0974 - val_accuracy: 0.9594\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.1042 - accuracy: 0.9633 - val_loss: 0.0493 - val_accuracy: 0.9824\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.1137 - accuracy: 0.9642 - val_loss: 0.0665 - val_accuracy: 0.9693\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.0991 - accuracy: 0.9718 - val_loss: 0.0398 - val_accuracy: 0.9857\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.1008 - accuracy: 0.9708 - val_loss: 0.0602 - val_accuracy: 0.9759\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 0.0930 - accuracy: 0.9699 - val_loss: 0.0533 - val_accuracy: 0.9824\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0815 - accuracy: 0.9751 - val_loss: 0.0365 - val_accuracy: 0.9846\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0792 - accuracy: 0.9755 - val_loss: 0.0366 - val_accuracy: 0.9857\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0775 - accuracy: 0.9727 - val_loss: 0.0479 - val_accuracy: 0.9813\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 0.0686 - accuracy: 0.9769 - val_loss: 0.0604 - val_accuracy: 0.9780\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0861 - accuracy: 0.9741 - val_loss: 0.0420 - val_accuracy: 0.9879\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.0695 - accuracy: 0.9784 - val_loss: 0.0213 - val_accuracy: 0.9923\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0765 - accuracy: 0.9722 - val_loss: 0.0239 - val_accuracy: 0.9901\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.0698 - accuracy: 0.9798 - val_loss: 0.0372 - val_accuracy: 0.9824\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0720 - accuracy: 0.9774 - val_loss: 0.0336 - val_accuracy: 0.9868\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.0580 - accuracy: 0.9812 - val_loss: 0.0359 - val_accuracy: 0.9835\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.0564 - accuracy: 0.9826 - val_loss: 0.0257 - val_accuracy: 0.9923\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0631 - accuracy: 0.9788 - val_loss: 0.0274 - val_accuracy: 0.9890\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.0503 - accuracy: 0.9835 - val_loss: 0.0290 - val_accuracy: 0.9879\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0602 - accuracy: 0.9812 - val_loss: 0.0314 - val_accuracy: 0.9846\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0584 - accuracy: 0.9807 - val_loss: 0.0469 - val_accuracy: 0.9813\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0684 - accuracy: 0.9765 - val_loss: 0.0290 - val_accuracy: 0.9857\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0478 - accuracy: 0.9835 - val_loss: 0.0281 - val_accuracy: 0.9879\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0561 - accuracy: 0.9821 - val_loss: 0.0485 - val_accuracy: 0.9857\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0496 - accuracy: 0.9854 - val_loss: 0.0263 - val_accuracy: 0.9912\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0597 - accuracy: 0.9802 - val_loss: 0.0212 - val_accuracy: 0.9923\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0115 - val_accuracy: 0.9956\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0534 - accuracy: 0.9807 - val_loss: 0.0147 - val_accuracy: 0.9956\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 7s 106ms/step - loss: 0.0420 - accuracy: 0.9864 - val_loss: 0.0453 - val_accuracy: 0.9879\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 7s 107ms/step - loss: 0.0491 - accuracy: 0.9807 - val_loss: 0.0281 - val_accuracy: 0.9868\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 6s 95ms/step - loss: 0.0476 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9868\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.0510 - accuracy: 0.9812 - val_loss: 0.0253 - val_accuracy: 0.9934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "test_loss, test_acc = model_vgg.evaluate(x_test_scaled, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# 예측값 계산\n",
        "y_pred = model_vgg.predict(x_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "Lxy2aKH-Yjv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b5cb54-2923-4e94-c59e-e555a6c714e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 4s 180ms/step - loss: 0.0171 - accuracy: 0.9947\n",
            "Test Accuracy: 0.9947368502616882\n",
            "24/24 [==============================] - 2s 63ms/step\n",
            "Confusion Matrix:\n",
            " [[ 8  0  0 ...  0  0  0]\n",
            " [ 0  7  0 ...  0  0  0]\n",
            " [ 0  0 23 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  7  0  0]\n",
            " [ 0  0  0 ...  0  5  0]\n",
            " [ 0  0  0 ...  0  0  4]]\n",
            "Classification Report:\n",
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "                     1 Reminazolam       1.00      1.00      1.00         8\n",
            "                     107 Halazepam       1.00      1.00      1.00         7\n",
            "                      11 Locaserin       1.00      1.00      1.00        23\n",
            "                    114 Flurazepam       1.00      1.00      1.00         5\n",
            "                 116 Flunitrazepam       1.00      1.00      1.00        15\n",
            "                       12 Quazepam       1.00      1.00      1.00        36\n",
            "                   128 Pentazocine       1.00      1.00      1.00        10\n",
            "               133 Phendimetrazine       1.00      1.00      1.00        19\n",
            "                      137 Pemoline       1.00      1.00      1.00        13\n",
            "                 138 Phenobarbital       1.00      1.00      1.00         4\n",
            "                   139 Fenetylline       1.00      1.00      1.00        12\n",
            "                     144 Triazolam       1.00      1.00      1.00        12\n",
            "                     147 Temazepam       1.00      1.00      1.00        13\n",
            "                   149 Clotiazepam       1.00      1.00      1.00         8\n",
            "                      150 Clobazam       1.00      1.00      1.00         8\n",
            "              152 Chlordiazepoxide       1.00      1.00      1.00        17\n",
            "                   155 Clorazepate       1.00      1.00      1.00        10\n",
            "                    156 Clonazepam       1.00      1.00      1.00         6\n",
            "                        157 Kratom       1.00      1.00      1.00         3\n",
            "                     161 Camazepam       0.95      1.00      0.97        19\n",
            "                  162 Carisoprodol       1.00      1.00      1.00        13\n",
            "                      166 Zipeprol       1.00      1.00      1.00         4\n",
            "                      168 Zolpidem       1.00      0.91      0.95        11\n",
            "                     169 Zopiclone       1.00      1.00      1.00        13\n",
            "                      170 Ibogaine       1.00      1.00      1.00        14\n",
            "             193 Ethyl Loflazepate       1.00      1.00      1.00        11\n",
            "                     194 Estazolam       1.00      1.00      1.00        13\n",
            "                 195 Ethchlorvynol       1.00      1.00      1.00        12\n",
            "                    198 Alprazolam       1.00      1.00      1.00        16\n",
            "                       22 Etizolam       1.00      1.00      1.00        38\n",
            "                    224 Bromazepam       1.00      1.00      1.00        16\n",
            "                 233 Benzphetamine       1.00      1.00      1.00         9\n",
            "                     238 Mexazolam       1.00      1.00      1.00         5\n",
            "                   239 Meprobamate       1.00      1.00      1.00        15\n",
            "               245 Methylphenidate       1.00      1.00      1.00        15\n",
            "                      253 Mazindol       1.00      1.00      1.00        21\n",
            "                     258 Lorazepam       1.00      1.00      1.00        20\n",
            "                      259 Diazepam       1.00      1.00      1.00        12\n",
            "28 Methylenedioxypyrovalerone,MDPV       1.00      1.00      1.00         5\n",
            "                    282 5-MeO-DiPT       1.00      1.00      1.00        14\n",
            "           29002 Opiqutan Soft Cap       1.00      1.00      1.00         3\n",
            "                          295 2C-B       1.00      1.00      1.00         5\n",
            "                           30 W-18       1.00      1.00      1.00        17\n",
            "                      303 Phenibut       1.00      1.00      1.00         9\n",
            "             34342 Welood Soft Cap       1.00      1.00      1.00         8\n",
            "                   37990 Multi-Q10       1.00      1.00      1.00        16\n",
            "               39916 Uniteride Tab       1.00      1.00      1.00         9\n",
            "                     40122 Uristat       1.00      1.00      1.00         9\n",
            "                40720 New Glia Tab       1.00      1.00      1.00        12\n",
            "                40767 Gestaren Tab       1.00      1.00      1.00         7\n",
            "                 40792 Diacell Cap       1.00      1.00      1.00         8\n",
            "                  40837 Duosta Tab       0.88      1.00      0.93         7\n",
            "              40949 Afental CR Tab       1.00      1.00      1.00         8\n",
            "           40953 Canagabarotin Cap       1.00      1.00      1.00        13\n",
            "              40990 Sebaco Hct Tab       1.00      1.00      1.00         9\n",
            "              40991 Sebaco Hct Tab       1.00      1.00      1.00        10\n",
            "      41097 Kyungdong Fanitine Tab       1.00      1.00      1.00         6\n",
            "                  41107 Anacox Cap       1.00      1.00      1.00         6\n",
            "                41169 Razarect Tab       1.00      1.00      1.00         9\n",
            "               41170 Leviepill Tab       1.00      0.57      0.73         7\n",
            "               41172 Levetiracetam       0.75      1.00      0.86         6\n",
            "                  41207 Vtamin Tab       1.00      1.00      1.00         8\n",
            "          41327 Rabeprazole Sodium       1.00      1.00      1.00         5\n",
            "                  41344 Tovast Tab       1.00      1.00      1.00        10\n",
            "                       59 4,4_DMAR       1.00      1.00      1.00        12\n",
            "                     78 Phenazepam       1.00      1.00      1.00         7\n",
            "                          9 AL-LAD       1.00      1.00      1.00         5\n",
            "                         94 5-mapb       1.00      1.00      1.00         4\n",
            "\n",
            "                          accuracy                           0.99       760\n",
            "                         macro avg       0.99      0.99      0.99       760\n",
            "                      weighted avg       1.00      0.99      0.99       760\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model_vgg.save('vgg16_transfer_1.h5')"
      ],
      "metadata": {
        "id": "H9m-E3FnYkeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d577b461-3e60-4e07-8fd0-2faf3589fa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DNN"
      ],
      "metadata": {
        "id": "wUfI5Q8fCuCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "x_train_scaled=x_train/255.0\n",
        "x_test_scaled=x_test/255.0\n",
        "x_train_scaled = x_train_scaled.reshape(-1,150*150*3)\n",
        "x_test_scaled=x_test_scaled.reshape(-1,150*150*3)"
      ],
      "metadata": {
        "id": "zbb_cfoiD7gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Sequential, Input\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense,Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wETZmvIZfU33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구성\n",
        "model_d = Sequential()\n",
        "model_d.add(Input(shape=(150*150*3, )))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "model_d.add(Dense(64,activation='relu'))\n",
        "model_d.add(Dense(32,activation='relu'))\n",
        "model_d.add(Dense(16,activation='relu'))\n",
        "model_d.add(Dense(32,activation='relu'))\n",
        "model_d.add(Dense(68,activation='softmax'))"
      ],
      "metadata": {
        "id": "03o-ucQ3CvNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_d.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics='accuracy')\n",
        "history = model_d.fit(x_train_scaled, y_train, epochs=100, batch_size=32,validation_split=0.3)"
      ],
      "metadata": {
        "id": "GXavrB_5DNWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc6f41c-2614-435c-9847-6699aec7f6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "67/67 [==============================] - 4s 26ms/step - loss: 4.5361 - accuracy: 0.0466 - val_loss: 4.0439 - val_accuracy: 0.0582\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 3.8915 - accuracy: 0.0814 - val_loss: 3.7775 - val_accuracy: 0.0615\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 3.6129 - accuracy: 0.1144 - val_loss: 3.4879 - val_accuracy: 0.1010\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 3.3658 - accuracy: 0.1289 - val_loss: 3.2618 - val_accuracy: 0.1175\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 3.1422 - accuracy: 0.1699 - val_loss: 3.0310 - val_accuracy: 0.1636\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 2.9560 - accuracy: 0.1896 - val_loss: 2.9083 - val_accuracy: 0.1998\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 2.8107 - accuracy: 0.2259 - val_loss: 2.7546 - val_accuracy: 0.2404\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 2.6708 - accuracy: 0.2593 - val_loss: 2.6257 - val_accuracy: 0.2602\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 2.5332 - accuracy: 0.2899 - val_loss: 2.4956 - val_accuracy: 0.2909\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 2.4765 - accuracy: 0.3045 - val_loss: 2.5090 - val_accuracy: 0.2766\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 2.3523 - accuracy: 0.3299 - val_loss: 2.3595 - val_accuracy: 0.3095\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 2.2730 - accuracy: 0.3511 - val_loss: 2.3706 - val_accuracy: 0.3008\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.2006 - accuracy: 0.3642 - val_loss: 2.1872 - val_accuracy: 0.3633\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 2.2268 - accuracy: 0.3704 - val_loss: 2.2063 - val_accuracy: 0.3666\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 1s 17ms/step - loss: 2.1330 - accuracy: 0.3722 - val_loss: 2.2635 - val_accuracy: 0.3414\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 2.1107 - accuracy: 0.3939 - val_loss: 2.2517 - val_accuracy: 0.3710\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 2.0203 - accuracy: 0.4108 - val_loss: 2.1280 - val_accuracy: 0.3919\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.9585 - accuracy: 0.4259 - val_loss: 1.9995 - val_accuracy: 0.4149\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.8978 - accuracy: 0.4409 - val_loss: 2.1515 - val_accuracy: 0.3853\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.9337 - accuracy: 0.4456 - val_loss: 2.0688 - val_accuracy: 0.3930\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.8284 - accuracy: 0.4640 - val_loss: 2.1354 - val_accuracy: 0.3930\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.8554 - accuracy: 0.4565 - val_loss: 1.9439 - val_accuracy: 0.4413\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.7309 - accuracy: 0.4969 - val_loss: 1.9040 - val_accuracy: 0.4248\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.6684 - accuracy: 0.5115 - val_loss: 1.7255 - val_accuracy: 0.4797\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.6260 - accuracy: 0.5106 - val_loss: 2.0134 - val_accuracy: 0.4007\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.7069 - accuracy: 0.4979 - val_loss: 2.0502 - val_accuracy: 0.4325\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.7280 - accuracy: 0.4776 - val_loss: 1.7521 - val_accuracy: 0.4808\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.5847 - accuracy: 0.5214 - val_loss: 1.7079 - val_accuracy: 0.4896\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 1s 17ms/step - loss: 1.5691 - accuracy: 0.5191 - val_loss: 1.6486 - val_accuracy: 0.4654\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 1.5640 - accuracy: 0.5271 - val_loss: 1.6585 - val_accuracy: 0.4852\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4329 - accuracy: 0.5515 - val_loss: 1.6262 - val_accuracy: 0.4918\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4326 - accuracy: 0.5609 - val_loss: 1.7576 - val_accuracy: 0.4962\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7040 - accuracy: 0.5045 - val_loss: 1.7891 - val_accuracy: 0.4632\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 2.0901 - accuracy: 0.4264 - val_loss: 2.0826 - val_accuracy: 0.3930\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.5520 - accuracy: 0.5341 - val_loss: 1.6537 - val_accuracy: 0.4676\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.4263 - accuracy: 0.5656 - val_loss: 1.6064 - val_accuracy: 0.5258\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.3965 - accuracy: 0.5671 - val_loss: 1.6288 - val_accuracy: 0.5225\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.5237 - accuracy: 0.5473 - val_loss: 1.6235 - val_accuracy: 0.4885\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.5039 - accuracy: 0.5351 - val_loss: 1.7446 - val_accuracy: 0.5038\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.4157 - accuracy: 0.5544 - val_loss: 1.5449 - val_accuracy: 0.5313\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.3551 - accuracy: 0.5840 - val_loss: 1.7020 - val_accuracy: 0.5203\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 1.4215 - accuracy: 0.5633 - val_loss: 1.7967 - val_accuracy: 0.4610\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.3315 - accuracy: 0.5835 - val_loss: 1.5140 - val_accuracy: 0.5247\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.2918 - accuracy: 0.5953 - val_loss: 1.5073 - val_accuracy: 0.5390\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.4892 - accuracy: 0.5605 - val_loss: 1.7657 - val_accuracy: 0.4731\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 1s 17ms/step - loss: 1.4148 - accuracy: 0.5572 - val_loss: 1.5228 - val_accuracy: 0.5324\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 1s 17ms/step - loss: 1.2708 - accuracy: 0.6042 - val_loss: 1.4949 - val_accuracy: 0.5357\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.2579 - accuracy: 0.5948 - val_loss: 1.4813 - val_accuracy: 0.5434\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3172 - accuracy: 0.5906 - val_loss: 1.9247 - val_accuracy: 0.4731\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.5013 - accuracy: 0.5482 - val_loss: 1.6614 - val_accuracy: 0.5159\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.5906 - accuracy: 0.5313 - val_loss: 1.7830 - val_accuracy: 0.5137\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.5632 - accuracy: 0.5384 - val_loss: 2.1673 - val_accuracy: 0.4325\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.5325 - accuracy: 0.5388 - val_loss: 1.7120 - val_accuracy: 0.5093\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.4399 - accuracy: 0.5445 - val_loss: 1.5673 - val_accuracy: 0.5104\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.4166 - accuracy: 0.5482 - val_loss: 1.5044 - val_accuracy: 0.5653\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.2669 - accuracy: 0.5944 - val_loss: 1.6043 - val_accuracy: 0.5291\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.3544 - accuracy: 0.5812 - val_loss: 1.5123 - val_accuracy: 0.5445\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.2185 - accuracy: 0.6108 - val_loss: 1.4550 - val_accuracy: 0.5598\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.1884 - accuracy: 0.6146 - val_loss: 1.4702 - val_accuracy: 0.5269\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.2076 - accuracy: 0.6071 - val_loss: 1.4810 - val_accuracy: 0.5631\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.2003 - accuracy: 0.6146 - val_loss: 1.3683 - val_accuracy: 0.5774\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 1s 17ms/step - loss: 1.2224 - accuracy: 0.6099 - val_loss: 1.3975 - val_accuracy: 0.5950\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 1.1908 - accuracy: 0.6141 - val_loss: 1.4026 - val_accuracy: 0.5675\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 1.1731 - accuracy: 0.6231 - val_loss: 1.4441 - val_accuracy: 0.5675\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 1.2740 - accuracy: 0.6009 - val_loss: 1.4530 - val_accuracy: 0.5598\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 1s 18ms/step - loss: 1.1526 - accuracy: 0.6231 - val_loss: 1.4645 - val_accuracy: 0.5642\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 1.1252 - accuracy: 0.6306 - val_loss: 1.3624 - val_accuracy: 0.5807\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 1.1384 - accuracy: 0.6188 - val_loss: 1.3345 - val_accuracy: 0.6004\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1572 - accuracy: 0.6231 - val_loss: 1.8847 - val_accuracy: 0.4874\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 1s 18ms/step - loss: 1.5823 - accuracy: 0.5379 - val_loss: 2.0724 - val_accuracy: 0.4610\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.6894 - accuracy: 0.4955 - val_loss: 2.2512 - val_accuracy: 0.3930\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 1.3309 - accuracy: 0.5741 - val_loss: 1.5974 - val_accuracy: 0.5587\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.1955 - accuracy: 0.6207 - val_loss: 1.5667 - val_accuracy: 0.5203\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.1742 - accuracy: 0.6221 - val_loss: 1.4095 - val_accuracy: 0.5785\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.1187 - accuracy: 0.6381 - val_loss: 1.4684 - val_accuracy: 0.5763\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1140 - accuracy: 0.6391 - val_loss: 1.3438 - val_accuracy: 0.6081\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1278 - accuracy: 0.6282 - val_loss: 1.3548 - val_accuracy: 0.6037\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1033 - accuracy: 0.6367 - val_loss: 1.4239 - val_accuracy: 0.5752\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.0727 - accuracy: 0.6419 - val_loss: 1.4315 - val_accuracy: 0.5960\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.0975 - accuracy: 0.6362 - val_loss: 1.5305 - val_accuracy: 0.5895\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.1356 - accuracy: 0.6282 - val_loss: 1.3810 - val_accuracy: 0.6004\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.0614 - accuracy: 0.6452 - val_loss: 1.3157 - val_accuracy: 0.5993\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.0479 - accuracy: 0.6541 - val_loss: 1.4464 - val_accuracy: 0.5653\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.2469 - accuracy: 0.6085 - val_loss: 1.5692 - val_accuracy: 0.5532\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.1241 - accuracy: 0.6376 - val_loss: 1.4712 - val_accuracy: 0.5873\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.2477 - accuracy: 0.6132 - val_loss: 1.3295 - val_accuracy: 0.6048\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.0846 - accuracy: 0.6353 - val_loss: 1.6067 - val_accuracy: 0.5620\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.1444 - accuracy: 0.6320 - val_loss: 1.4531 - val_accuracy: 0.5873\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.0745 - accuracy: 0.6452 - val_loss: 1.2923 - val_accuracy: 0.6301\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 1.0492 - accuracy: 0.6579 - val_loss: 1.3330 - val_accuracy: 0.5993\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 1.1169 - accuracy: 0.6348 - val_loss: 1.4449 - val_accuracy: 0.5510\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0395 - accuracy: 0.6616 - val_loss: 1.2402 - val_accuracy: 0.6367\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 1.0470 - accuracy: 0.6536 - val_loss: 1.2635 - val_accuracy: 0.6498\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.0534 - accuracy: 0.6461 - val_loss: 1.4635 - val_accuracy: 0.5818\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1238 - accuracy: 0.6414 - val_loss: 1.4967 - val_accuracy: 0.5499\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 1.1371 - accuracy: 0.6268 - val_loss: 1.1945 - val_accuracy: 0.6663\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.9640 - accuracy: 0.6894 - val_loss: 1.1978 - val_accuracy: 0.6531\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.9761 - accuracy: 0.6673 - val_loss: 1.2899 - val_accuracy: 0.6081\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.9824 - accuracy: 0.6687 - val_loss: 1.2292 - val_accuracy: 0.6279\n",
            "Epoch 100/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.9546 - accuracy: 0.6856 - val_loss: 1.2396 - val_accuracy: 0.6476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model_d.evaluate(x_test_scaled, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# 예측값 계산\n",
        "y_pred = model_d.predict(x_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred_classes)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yw1kgMvKXIM",
        "outputId": "38048ab5-781e-4514-b62a-c863b4e5ec03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 6ms/step - loss: 1.2788 - accuracy: 0.6434\n",
            "Test Accuracy: 0.6434210538864136\n",
            "24/24 [==============================] - 0s 6ms/step\n",
            "Confusion Matrix:\n",
            " [[8 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 5 0]\n",
            " [0 0 0 ... 0 0 4]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         8\n",
            "           1       0.00      0.00      0.00         7\n",
            "           2       0.00      0.00      0.00        23\n",
            "           3       1.00      1.00      1.00         5\n",
            "           4       1.00      1.00      1.00        15\n",
            "           5       1.00      1.00      1.00        36\n",
            "           6       0.00      0.00      0.00        10\n",
            "           7       0.11      0.37      0.16        19\n",
            "           8       1.00      1.00      1.00        13\n",
            "           9       0.00      0.00      0.00         4\n",
            "          10       0.00      0.00      0.00        12\n",
            "          11       0.00      0.00      0.00        12\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.20      0.62      0.30         8\n",
            "          14       0.00      0.00      0.00         8\n",
            "          15       1.00      1.00      1.00        17\n",
            "          16       0.00      0.00      0.00        10\n",
            "          17       1.00      1.00      1.00         6\n",
            "          18       0.00      0.00      0.00         3\n",
            "          19       0.80      0.42      0.55        19\n",
            "          20       0.00      0.00      0.00        13\n",
            "          21       0.00      0.00      0.00         4\n",
            "          22       0.00      0.00      0.00        11\n",
            "          23       0.00      0.00      0.00        13\n",
            "          24       0.59      0.93      0.72        14\n",
            "          25       0.92      1.00      0.96        11\n",
            "          26       0.86      0.92      0.89        13\n",
            "          27       1.00      0.92      0.96        12\n",
            "          28       1.00      0.88      0.93        16\n",
            "          29       1.00      1.00      1.00        38\n",
            "          30       1.00      1.00      1.00        16\n",
            "          31       1.00      1.00      1.00         9\n",
            "          32       1.00      1.00      1.00         5\n",
            "          33       1.00      1.00      1.00        15\n",
            "          34       0.40      0.53      0.46        15\n",
            "          35       0.00      0.00      0.00        21\n",
            "          36       1.00      1.00      1.00        20\n",
            "          37       0.04      0.42      0.08        12\n",
            "          38       1.00      1.00      1.00         5\n",
            "          39       0.61      1.00      0.76        14\n",
            "          40       1.00      0.33      0.50         3\n",
            "          41       1.00      1.00      1.00         5\n",
            "          42       0.00      0.00      0.00        17\n",
            "          43       1.00      1.00      1.00         9\n",
            "          44       1.00      1.00      1.00         8\n",
            "          45       0.81      0.81      0.81        16\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         9\n",
            "          48       0.92      1.00      0.96        12\n",
            "          49       1.00      1.00      1.00         7\n",
            "          50       0.62      0.62      0.62         8\n",
            "          51       1.00      0.57      0.73         7\n",
            "          52       0.54      0.88      0.67         8\n",
            "          53       1.00      0.77      0.87        13\n",
            "          54       0.88      0.78      0.82         9\n",
            "          55       0.50      0.60      0.55        10\n",
            "          56       0.86      1.00      0.92         6\n",
            "          57       0.71      0.83      0.77         6\n",
            "          58       1.00      1.00      1.00         9\n",
            "          59       0.86      0.86      0.86         7\n",
            "          60       0.83      0.83      0.83         6\n",
            "          61       1.00      1.00      1.00         8\n",
            "          62       1.00      1.00      1.00         5\n",
            "          63       0.67      0.40      0.50        10\n",
            "          64       0.18      0.33      0.24        12\n",
            "          65       0.00      0.00      0.00         7\n",
            "          66       0.83      1.00      0.91         5\n",
            "          67       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.64       760\n",
            "   macro avg       0.63      0.64      0.62       760\n",
            "weighted avg       0.63      0.64      0.62       760\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet"
      ],
      "metadata": {
        "id": "dD3HyAKYrrnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
        "\n",
        "resnet_base.summary()"
      ],
      "metadata": {
        "id": "CaFDZAmRrwAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19509d3b-bcbc-4ddf-d409-86b79f27688e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 150, 150, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 156, 156, 3)          0         ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 75, 75, 64)           9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 75, 75, 64)           256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 75, 75, 64)           0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 77, 77, 64)           0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 38, 38, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 38, 38, 64)           4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 38, 38, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 38, 38, 256)          16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 38, 38, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 38, 38, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 38, 38, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 38, 38, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 38, 38, 256)          0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 38, 38, 64)           16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 38, 38, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 38, 38, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 38, 38, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 38, 38, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 38, 38, 256)          0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 38, 38, 64)           16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 38, 38, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 38, 38, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 38, 38, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 38, 38, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 38, 38, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 38, 38, 256)          0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 38, 38, 256)          0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 19, 19, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 19, 19, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 19, 19, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 19, 19, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 19, 19, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 19, 19, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 19, 19, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 19, 19, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 19, 19, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 19, 19, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 19, 19, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 19, 19, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 19, 19, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 19, 19, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 19, 19, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 19, 19, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 19, 19, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 19, 19, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 19, 19, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 19, 19, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 19, 19, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 19, 19, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 19, 19, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 19, 19, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 19, 19, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 19, 19, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 19, 19, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 19, 19, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 10, 10, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 10, 10, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 10, 10, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 10, 10, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 10, 10, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 10, 10, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 10, 10, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 10, 10, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 10, 10, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 10, 10, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 10, 10, 1024)         263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 10, 10, 1024)         4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 10, 10, 1024)         0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 10, 10, 1024)         0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 5, 5, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 5, 5, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 5, 5, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 5, 5, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 5, 5, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 5, 5, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 5, 5, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 5, 5, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 5, 5, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 5, 5, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 5, 5, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 5, 5, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 5, 5, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 5, 5, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 5, 5, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 5, 5, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 5, 5, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 5, 5, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 5, 5, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 5, 5, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 5, 5, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 5, 5, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23587712 (89.98 MB)\n",
            "Trainable params: 23534592 (89.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.applications import ResNet50\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model_resnet = Sequential()\n",
        "model_resnet.add(resnet_base)\n",
        "model_resnet.add(layers.Flatten())\n",
        "model_resnet.add(layers.Dense(256, activation='relu'))\n",
        "model_resnet.add(layers.Dropout(0.5))\n",
        "model_resnet.add(layers.Dense(68, activation='softmax'))\n",
        "\n",
        "# ResNet50의 Convolutional Layer는 이미 학습되어 있으므로 학습되지 않도록 설정\n",
        "resnet_base.trainable = False\n",
        "\n",
        "# 모델 컴파일\n",
        "model_resnet.compile(loss='sparse_categorical_crossentropy', optimizer=RMSprop(learning_rate=1e-4), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "javQ4u20rw9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# ModelCheckpoint 콜백 생성\n",
        "checkpoint_path = \"best_model_resnet.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# EarlyStopping 콜백 생성\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습 시 ModelCheckpoint와 EarlyStopping 콜백 지정\n",
        "history_resnet = model_resnet.fit(x_train, y_train, epochs=50, batch_size=512, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
      ],
      "metadata": {
        "id": "g0r9-6iSr2WC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbea3b3c-674e-41f1-9b39-9a01e0272acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3143 - accuracy: 0.2055\n",
            "Epoch 1: val_loss improved from -inf to 1.28952, saving model to best_model_resnet.h5\n",
            "5/5 [==============================] - 10s 1s/step - loss: 4.3143 - accuracy: 0.2055 - val_loss: 1.2895 - val_accuracy: 0.7253\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2837 - accuracy: 0.6664\n",
            "Epoch 2: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 4s 943ms/step - loss: 1.2837 - accuracy: 0.6664 - val_loss: 0.4787 - val_accuracy: 0.8717\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.8340\n",
            "Epoch 3: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 977ms/step - loss: 0.5987 - accuracy: 0.8340 - val_loss: 0.2343 - val_accuracy: 0.9375\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.9028\n",
            "Epoch 4: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 969ms/step - loss: 0.3918 - accuracy: 0.9028 - val_loss: 0.2146 - val_accuracy: 0.9441\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9304\n",
            "Epoch 5: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.2666 - accuracy: 0.9304 - val_loss: 0.1431 - val_accuracy: 0.9507\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9596\n",
            "Epoch 6: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.1651 - accuracy: 0.9596 - val_loss: 0.1167 - val_accuracy: 0.9638\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9671\n",
            "Epoch 7: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.1351 - accuracy: 0.9671 - val_loss: 0.0803 - val_accuracy: 0.9786\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9687\n",
            "Epoch 8: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 959ms/step - loss: 0.1211 - accuracy: 0.9687 - val_loss: 0.0978 - val_accuracy: 0.9688\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9613\n",
            "Epoch 9: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.1412 - accuracy: 0.9613 - val_loss: 0.0635 - val_accuracy: 0.9836\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9909\n",
            "Epoch 10: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.0529 - accuracy: 0.9909 - val_loss: 0.0702 - val_accuracy: 0.9819\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9815\n",
            "Epoch 11: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.0651 - accuracy: 0.9815 - val_loss: 0.1643 - val_accuracy: 0.9408\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.9287\n",
            "Epoch 12: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.3298 - accuracy: 0.9287 - val_loss: 0.0489 - val_accuracy: 0.9868\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9893\n",
            "Epoch 13: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 5s 958ms/step - loss: 0.0514 - accuracy: 0.9893 - val_loss: 0.0360 - val_accuracy: 0.9868\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9934\n",
            "Epoch 14: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 4s 913ms/step - loss: 0.0309 - accuracy: 0.9934 - val_loss: 0.0496 - val_accuracy: 0.9786\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9938\n",
            "Epoch 15: val_loss did not improve from 1.28952\n",
            "5/5 [==============================] - 4s 913ms/step - loss: 0.0307 - accuracy: 0.9938 - val_loss: 0.0432 - val_accuracy: 0.9836\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9959\n",
            "Epoch 16: val_loss did not improve from 1.28952\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.0202 - accuracy: 0.9959 - val_loss: 0.0428 - val_accuracy: 0.9803\n",
            "Epoch 16: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 모델 불러오기\n",
        "#model_resnet = tf.keras.models.load_model('best_model_resnet.h5')\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model_resnet.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# 예측값 계산\n",
        "y_pred = model_resnet.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRYRgMl_v_zJ",
        "outputId": "43dc7bae-4db2-479f-a2ce-c0ff62e814ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 54ms/step - loss: 0.0512 - accuracy: 0.9842\n",
            "Test Accuracy: 0.9842105507850647\n",
            "24/24 [==============================] - 2s 50ms/step\n",
            "Confusion Matrix:\n",
            " [[ 8  0  0 ...  0  0  0]\n",
            " [ 0  7  0 ...  0  0  0]\n",
            " [ 0  0 23 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  7  0  0]\n",
            " [ 0  0  0 ...  0  5  0]\n",
            " [ 0  0  0 ...  0  0  4]]\n",
            "Classification Report:\n",
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "                     1 Reminazolam       1.00      1.00      1.00         8\n",
            "                     107 Halazepam       1.00      1.00      1.00         7\n",
            "                      11 Locaserin       1.00      1.00      1.00        23\n",
            "                    114 Flurazepam       1.00      1.00      1.00         5\n",
            "                 116 Flunitrazepam       1.00      1.00      1.00        15\n",
            "                       12 Quazepam       1.00      1.00      1.00        36\n",
            "                   128 Pentazocine       0.77      1.00      0.87        10\n",
            "               133 Phendimetrazine       1.00      0.84      0.91        19\n",
            "                      137 Pemoline       1.00      1.00      1.00        13\n",
            "                 138 Phenobarbital       1.00      1.00      1.00         4\n",
            "                   139 Fenetylline       1.00      1.00      1.00        12\n",
            "                     144 Triazolam       1.00      1.00      1.00        12\n",
            "                     147 Temazepam       1.00      1.00      1.00        13\n",
            "                   149 Clotiazepam       0.89      1.00      0.94         8\n",
            "                      150 Clobazam       1.00      0.88      0.93         8\n",
            "              152 Chlordiazepoxide       1.00      1.00      1.00        17\n",
            "                   155 Clorazepate       1.00      1.00      1.00        10\n",
            "                    156 Clonazepam       1.00      1.00      1.00         6\n",
            "                        157 Kratom       1.00      1.00      1.00         3\n",
            "                     161 Camazepam       1.00      1.00      1.00        19\n",
            "                  162 Carisoprodol       1.00      1.00      1.00        13\n",
            "                      166 Zipeprol       1.00      1.00      1.00         4\n",
            "                      168 Zolpidem       1.00      1.00      1.00        11\n",
            "                     169 Zopiclone       1.00      1.00      1.00        13\n",
            "                      170 Ibogaine       1.00      1.00      1.00        14\n",
            "             193 Ethyl Loflazepate       1.00      1.00      1.00        11\n",
            "                     194 Estazolam       1.00      1.00      1.00        13\n",
            "                 195 Ethchlorvynol       1.00      1.00      1.00        12\n",
            "                    198 Alprazolam       1.00      1.00      1.00        16\n",
            "                       22 Etizolam       1.00      1.00      1.00        38\n",
            "                    224 Bromazepam       1.00      1.00      1.00        16\n",
            "                 233 Benzphetamine       1.00      1.00      1.00         9\n",
            "                     238 Mexazolam       1.00      1.00      1.00         5\n",
            "                   239 Meprobamate       1.00      1.00      1.00        15\n",
            "               245 Methylphenidate       1.00      1.00      1.00        15\n",
            "                      253 Mazindol       1.00      1.00      1.00        21\n",
            "                     258 Lorazepam       1.00      1.00      1.00        20\n",
            "                      259 Diazepam       1.00      1.00      1.00        12\n",
            "28 Methylenedioxypyrovalerone,MDPV       1.00      1.00      1.00         5\n",
            "                    282 5-MeO-DiPT       1.00      1.00      1.00        14\n",
            "           29002 Opiqutan Soft Cap       1.00      1.00      1.00         3\n",
            "                          295 2C-B       1.00      1.00      1.00         5\n",
            "                           30 W-18       1.00      1.00      1.00        17\n",
            "                      303 Phenibut       1.00      1.00      1.00         9\n",
            "             34342 Welood Soft Cap       1.00      1.00      1.00         8\n",
            "                   37990 Multi-Q10       1.00      1.00      1.00        16\n",
            "               39916 Uniteride Tab       1.00      1.00      1.00         9\n",
            "                     40122 Uristat       1.00      1.00      1.00         9\n",
            "                40720 New Glia Tab       1.00      1.00      1.00        12\n",
            "                40767 Gestaren Tab       1.00      1.00      1.00         7\n",
            "                 40792 Diacell Cap       0.62      1.00      0.76         8\n",
            "                  40837 Duosta Tab       1.00      1.00      1.00         7\n",
            "              40949 Afental CR Tab       1.00      0.88      0.93         8\n",
            "           40953 Canagabarotin Cap       1.00      0.62      0.76        13\n",
            "              40990 Sebaco Hct Tab       1.00      1.00      1.00         9\n",
            "              40991 Sebaco Hct Tab       1.00      1.00      1.00        10\n",
            "      41097 Kyungdong Fanitine Tab       1.00      1.00      1.00         6\n",
            "                  41107 Anacox Cap       0.86      1.00      0.92         6\n",
            "                41169 Razarect Tab       1.00      1.00      1.00         9\n",
            "               41170 Leviepill Tab       0.88      1.00      0.93         7\n",
            "               41172 Levetiracetam       1.00      0.67      0.80         6\n",
            "                  41207 Vtamin Tab       1.00      1.00      1.00         8\n",
            "          41327 Rabeprazole Sodium       1.00      1.00      1.00         5\n",
            "                  41344 Tovast Tab       0.91      1.00      0.95        10\n",
            "                       59 4,4_DMAR       1.00      1.00      1.00        12\n",
            "                     78 Phenazepam       1.00      1.00      1.00         7\n",
            "                          9 AL-LAD       1.00      1.00      1.00         5\n",
            "                         94 5-mapb       1.00      1.00      1.00         4\n",
            "\n",
            "                          accuracy                           0.98       760\n",
            "                         macro avg       0.98      0.98      0.98       760\n",
            "                      weighted avg       0.99      0.98      0.98       760\n",
            "\n"
          ]
        }
      ]
    }
  ]
}